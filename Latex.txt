\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, top=30mm, bottom=30mm, left=20mm, right=20mm]{geometry}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{soul}
\usepackage{cite}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{pythonhighlight}
\usepackage{graphicx}
\newcommand{\pyobject}[1]{\ovalbox{\color{red}{\texttt{#1}}}}
\title{\Huge Metodi Del Calcolo Scientifico - Project 2 \\ \Large Academic Year 2019/2020}
\author{Simone Paolo Mottadelli, 820786}



\date{}
\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
\large
\section{Introduction}
\label{sec:introduction}
The main purpose of this project was to use the Discrete Cosine Transform 2 (\textbf{DCT2}) implementation in an open source environment in order to assess the effects of a compression algorithm on gray scale images. \\
In particular, this report consists of two parts. The first one describes a possible implementation of the DCT2 as presented during the lectures and shows the results obtained from the comparison between such implementation and the one provided by a an open source library in terms of computational complexity, while the second part focuses on the implementation of a compression algorithm for gray scale images, which does not use a quantization matrix but is very similar to the JPEG algorithm.
\section{First part}
\label{sec:firstpart}
The first part of the project has been implemented using the \textbf{Python} \cite{van1995python} programming language because it is open source, very powerful and it provides all the functionalities and libraries needed to achieve the goals of the project. More in detail, three libraries have been used:
\begin{itemize}
    \item \textbf{SciPy} \cite{2020SciPy-NMeth}, which is a Python-based ecosystem of open-source software for mathematics, science and engineering. It contains a fast implementation of the DCT2;
    \item \textbf{NumPy} \cite{numpy}, which is the core library for scientific computing in Python and which mainly adds support for multidimensional arrays;
    \item \textbf{Matplotlib} \cite{Hunter:2007}, which is a comprehensive library for creating static, animated and interactive visualizations in Python \cite{van1995python}.
\end{itemize}
The code has been organized into three modules:
\begin{itemize}
    \item \textbf{my\_dct2.py}, containing my implementation of the DCT2;
    \item \textbf{comparator.py}, containing the logic to compare my DCT2 implementation with the one of the SciPy library;
    \item \textbf{results.py}, containing the logic to save the results of the comparison on a file and plot the results.
\end{itemize}
Note that in order not to make this report too complicated and technical, only the module containing my implementation of the DCT2 will be described in depth, while for the other modules, just a brief description will be provided. However, all the code is available at \url{https://github.com/SimoneMottadelli/MetodiDelCalcoloScientifico}
\subsection{Implementation of the DCT2}
\label{subsec:implementation_of_my_dct2}
The \textbf{my\_dct2.py} module contains my implementation of the DCT2, which has been realized by applying the DCT1 first on the rows and then on the columns of the matrix in input. This design choice is related to the fact that the "brute-force" implementation of the DCT2 using the following formula has a time complexity equals to $O(n^{4})$:
$$c_{kl} = \frac{1}{\sqrt{\alpha_k}\sqrt{\alpha_l}}\sum_{i=1}^{n}\sum_{j=1}^{n} cos\left(\pi k \frac{2i-1}{2n}\right) cos\left(\pi l \frac{2j-1}{2n}\right)m_{ij}$$ 
where $$c, m \in \mathbb{R}^{n\times n} \:\:\:\:\:\: \text{and} \:\:\:\:\:\:
\alpha_k = \begin{cases} 
      n & k = 0 \\
      \frac{n}{2} & k\neq 0
   \end{cases}
$$
Instead, applying the DCT1 with the following formulas on the matrix in input, first on its rows and then on its columns, has a time complexity equals to $O(n^{3})$:
$$c_k = \frac{1}{\sqrt{\alpha_k}}\sum_{i=1}^{n}cos\left(\pi k \frac{2i-1}{2n}\right)m_i$$ 
where $$c, m \in \mathbb{R}^{n} \:\:\:\:\:\: \text{and} \:\:\:\:\:\:
\alpha_k = \begin{cases} 
      n & k = 0 \\
      \frac{n}{2} & k\neq 0
   \end{cases}
$$
Concerning the code, Figure \ref{fig:my_dct2} shows the content of the \textbf{my\_dct2.py} module. In particular, it contains two functions:
\begin{itemize}
    \item \textbf{my\_dct(m)}, which receives an \textbf{m} matrix in input and simply computes the DCT1 for each row of \textbf{m};
    \item \textbf{my\_dct2(m)}, which receives an \textbf{m} matrix in input and calls \textbf{my\_dct(m)} twice, first on \textbf{m} and then on the resulting transposed matrix.
\end{itemize}
\newpage
\begin{figure}
    \centering
    \begin{python}
import numpy as np
from math import cos, sqrt

# input: m (a numpy matrix)
def my_dct(m):
    nrows = m.shape[0]
    ncols = m.shape[1]
    c = np.zeros((nrows, ncols))
    for row in range(0, nrows):
        for k in range(0, ncols):
            total_sum = 0
            for i in range(1, ncols + 1):
                total_sum += cos(np.pi * k * (2 * i - 1) / (2 * ncols)) * 
                              m[row][i - 1]
            alpha_k = ncols if k == 0 else ncols * 0.5
            
            c[row][k] = total_sum / sqrt(alpha_k)
    return c

# input: m (a numpy matrix)
def my_dct2(m):
    return my_dct(my_dct(m).transpose()).transpose()
\end{python}
    \caption{My DCT2 implementation}
    \label{fig:my_dct2}
\end{figure}
\subsection{Comparison with the SciPy library}
After having implemented the DCT2 as described in Subsection \ref{subsec:implementation_of_my_dct2}, an empirical evaluation study has been conducted with the aim of comparing my DCT2 implementation with the one provided by the SciPi \cite{2020SciPy-NMeth} library in terms of their computational complexity. \\ 
First of all, the two algorithms have been executed with square matrices of increasing dimension and their executing time has been registered. This has been achieved by implementing the code inside the \textit{comparator.py} module, whose logic is very simple:
\begin{enumerate}
    \item It generates a random matrix of dimension $N\times N$
    \item It registers the execution time of my DCT2 algorithm
    \item It registers the execution time of the DCT2 of the SciPy library
    \item it returns to step 1 until the maximum predetermined dimension of the matrices has been reached
\end{enumerate}
To conduce the experiments, the maximum predetermined dimension of the matrices has been set to $N = 1000$ and the results have been collected using the \textbf{results.py} module, containing the logic for saving the results of each experiment on a file and for plotting the results on a semi-logarithmic scale.\\
Note that the actual execution time of an algorithm is strongly influenced by the scheduler of the operating system, which decides how much CPU time to allocate to each process. To mitigate this problem, 3 runs of the experiments have been conducted and the merged results are shown in Figure \ref{}. In particular, the plot shows that my implementation of the DCT2 has a cubic time complexity, while the algorithm provided by the SciPy \cite{2020SciPy-NMeth} library is much more efficient, since it provides a faster implementation of the DCT2 (\textbf{FFT}). In addition, it is possible to see that the curve generated by the algorithm of the SciPy \cite{2020SciPy-NMeth} library is much more irregular and this may depend on its actual implementation.
\section{Second part}
This second part of the project has also been implemented using the Python \cite{van1995python} programming language for the same reasons described in Section \ref{sec:firstpart}. \\
Moreover, since this part of the project required to work on \textbf{.bmp} images, an additional library, \textbf{Pillow} \cite{pillow}, has been used for opening and manipulating the images in input. \\
From an architectural point of view, the software has been organized into the following modules:
\begin{itemize}
    \item \textbf{main.py}, which is the entry point of the software;
    \item \textbf{input\_parser.py}, which implements the logic for parsing the user input, that is, the \textbf{F} and \textbf{d} parameter as well as the \textbf{path} in the file system of where the image file is located;
    \item \textbf{image\_processing.py}, which implements the compression algorithm for the gray scale images;
    \item \textbf{results.py}, which realizes the logic for showing the results of the compression algorithm to the user, i.e., it plots the image in input and the compressed image side by side at the end of the execution.
\end{itemize}
It is important to underline another time that also for this part of the project only the most important parts of the code will be shown in the document, but the whole software is available at the following URL: \url{https://github.com/SimoneMottadelli/MetodiDelCalcoloScientifico}.
\subsection{Software functioning}
\label{softwarefunctioning}
To execute the program, the user has to open a CMD window and enter the following command:
\begin{lstlisting}[basicstyle=\small\centering]
> python main.py -f <int value> -d <int value> -path <filepath>
\end{lstlisting}
where
\begin{itemize}
    \item \textbf{-f} is the option to specify the value of the \textbf{F} parameter, that is, the dimension of the Minimum Coded Unit (MCU) on which the DCT2 is executed;
    \item \textbf{-d} is the option to specify the value of the \textbf{d} parameter, that is, the cutoff threshold of the frequencies;
    \item \textbf{-path} is the option to specify the location of the image in the file system.
\end{itemize}
For example, the following command executes the program on an image with $F=8$ and $d=6$:
\begin{lstlisting}[basicstyle=\small\centering]
> python main.py -f 8 -d 6 -path image.bmp
\end{lstlisting}
More in detail, when the program is executed, the user input is validated by the \textbf{input\_parser.py} module, which first checks that all the required parameters have been entered and then checks the admissibility of the values of the parameters. For instance, it checks whether the value of the d parameter falls in the interval [0, 2F - 2] or whether the image exists in the file system. \\
After the input has been validated, the image is loaded from the file system, stored in a matrix and processed by the \textbf{image\_processing.py} module. The latter encapsulates the heart of the compression algorithm and is shown in Figure \ref{fig:image_proc1} and in Figure \ref{fig:imageproc2}.\\
In particular, the \textbf{process\_image(img\_to\_process , f, d)} function, shown in Figure \ref{fig:image_proc1}, receives in input the image to process and the values of the $F$ and $d$ parameters and it applies the following steps on each MCU of dimension FxF starting from the upper-left corner of the image: 
\begin{enumerate}
    \item It applies the DCT2 of the SciPy \cite{2020SciPy-NMeth} library on the MCU;
    \item For each coefficient of the MCU, it sets it to zero if its position, which is identified by the pair of indices $(k, l)$, is such that $k + l \geq d$;
    \item It applies the IDCT2 of the SciPy \cite{2020SciPy-NMeth} library on the MCU;
    \item it rounds the coefficients of the MCU to the nearest integer value and it adjusts their values to make them fall in the [0, 255] interval.
\end{enumerate}
\begin{figure}[h!]
    \centering
    \begin{python}
from scipy import fft

# input: the image to process
# input: the value of the F parameter 
# input: the value of the d parameter
def process_image(img_to_process, f, d):

    # work on a copy of the image in input
    img = img_to_process.copy()
    
    # get the dimensions of the image
    nrows = img.shape[0]
    ncols = img.shape[1]
    
    # compute the DCT2 on each MCU starting from the upper-left corner 
    # of the image
    for start_mcu_row in range(0, nrows, f):
        for start_mcu_col in range(0, ncols, f):

            # find the pixel positions to identify a MCU
            end_mcu_row = start_mcu_row + f
            end_mcu_col = start_mcu_col + f

            # extract the mcu from the image
            mcu = img[start_mcu_row:end_mcu_row, start_mcu_col:end_mcu_col]

            # if the mcu is a FxF square matrix, then compress it
            if mcu.shape[0] == f and mcu.shape[1] == f:
                mcu = fft.dctn(mcu, type=2, norm="ortho")
                mcu = cutoff_frequencies(mcu, d)
                mcu = fft.idctn(mcu, type=2, norm="ortho")
                mcu = adjust_coefficients(mcu)

            # save the processed mcu in its original position in the image
            img[start_mcu_row:end_mcu_row, start_mcu_col:end_mcu_col] = mcu

    return img

    \end{python}
    \caption{Implementation of the compression algorithm}
    \label{fig:image_proc1}
\end{figure}
This function makes use of the two helper functions shown in Figure \ref{fig:imageproc2}, which implement the aforementioned steps 2 and 4, respectively.\\
Note that when the division of the image into square blocks of pixels of size $F \times F$ occurs, i.e., when the MCU are generated, there might be some leftovers if the size of the image is not divisible by $F$. Therefore, the algorithm simply ignores those leftovers, but does not delete them from the image.  However, this should not be noticed by the human eye if a value for $F$ is chosen to be small compared to the image size. \\
After having processed the image, the \textbf{results.py} module is responsible for showing the results of the computation to the user, that is, it displays the image before and after the compression in an interactive screen, where the user can zoom in or zoom out the images to validate the quality of the compression, as also shown in Figure \ref{fig:exampleoutput}.
\begin{figure}
    \centering
    \begin{python}
# This function is used to cutoff the frequencies of the MCU in input
def cutoff_frequencies(mcu, d):
    for k in range(0, mcu.shape[0]):
        for l in range(0, mcu.shape[1]):
            if k + l >= d:
                mcu[k, l] = 0
    return mcu

# This function is used to round the coefficients of the MCU and
#adjusts them in such a way that their values fall in the [0, 255] interval
def adjust_coefficients(mcu):
    for i in range(0, mcu.shape[0]):
        for j in range(0, mcu.shape[1]):
            mcu[i, j] = max(0, min(round(mcu[i, j]), 255))
    return mcu
    \end{python}
    \caption{Helper functions used by the compression algorithm}
    \label{fig:imageproc2}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[width=0.81\textwidth]{example_output.png}
    \caption{Example of output}
    \label{fig:exampleoutput}
\end{figure}
\newpage
\subsection{Experiments}
This subsection presents some experiments that have been done to evaluate the effectiveness of the compression algorithm presented in Subsection \ref{softwarefunctioning}. \\
In particular, the compression algorithm seems to work quite well on images that are well nuanced, even if a low value of the $d$ parameter is chosen, as shown in Figure \ref{fig:ex1}, where the values chosen for the parameters are $F=10$ and $d=3$. The two images are almost identical.\\
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{ex1.png}
    \caption{Example with a nuanced image ($F=10$, $d=3$)}
    \label{fig:ex1}
\end{figure}
Instead, the same parameter setting on high contrast images leads to a greater lose of information; infact, some artifacts and noise are generated, as shown in Figure \ref{fig:ex2}. This is related to the Gibbs phenomenon. \\
The Gibbs phenomenon becomes more pronounced if the value of the $F$ parameter is big, while it reduces if the value is small. This idea is best clarified by looking at Figures \ref{fig:gibs1} and \ref{fig:gibs2}. \\
It's worthwhile to mention also the boundary cases, such as using a value equals to zero for the $d$ parameter, which leads to a complete lose of information and, thus, the compressed image would result black, as shown in Figure \ref{fig:ex3}, and the case of using the maximum value of $d$, which bring to obtaining an image that is not compressed at all, as shown Figure \ref{fig:ex4}. \\
Finally, as a last extreme case, if we use a huge value for $F$, bigger than the size of the image, for how the algorithm was implemented, the whole image is a leftover and no compression is applied since no MCU blocks exist. Thus, the image is unmodified, as shown by an example in Figure \ref{fig:Fhuge}.
\begin{figure}
\centering
\begin{subfigure}[b]{0.80\textwidth}
   \includegraphics[width=1\linewidth]{ex2.png}
   \caption{}
   \label{fig:Ng1} 
\end{subfigure}
\begin{subfigure}[b]{0.80\textwidth}
   \includegraphics[width=1\linewidth]{ex2_1.png}
   \caption{}
   \label{fig:Ng2}
\end{subfigure}
\caption{(a) Example with a high contrast image ($F=10$, $d=3$). (b) The same as (a) but zoomed in to show the artifacts introduced by the compression algorithm}
\label{fig:ex2}
\end{figure}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{gibbs1.png}
    \caption{Gibbs phenomenon when ($F=30$, $d=3$)}
    \label{fig:gibs1}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{gibbs2.png}
    \caption{Gibbs phenomenon when ($F=3$, $d=3$)}
    \label{fig:gibs2}
\end{figure}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{ex3.png}
    \caption{Boundary case: minimum value of $d$ ($F=10$, $d=0$)}
    \label{fig:ex3}
\end{figure}
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{ex4.png}
    \caption{Boundary case: maximum value of $d$ ($F=10$, $d=18$)}
    \label{fig:ex4}
\end{figure}
\newpage
\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{Fhuge.png}
    \caption{Extreme case: huge value of $F=30000$}
    \label{fig:Fhuge}
\end{figure}

\newpage
\bibliography{references}
\bibliographystyle{unsrt}
\end{document}